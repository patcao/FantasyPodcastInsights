{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fbbc8d5-6e7c-403d-838a-2ab1e2e7cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 400)\n",
    "\n",
    "from src.data_utils import load_clean_scores\n",
    "from src.player_utils import PlayerUtil\n",
    "\n",
    "seasons = [\"2023-24\"]\n",
    "df = load_clean_scores(seasons=seasons)\n",
    "player_util = PlayerUtil(seasons=seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6cb291fc-c4f4-462c-887d-d97e83110a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_date</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-18 16:25:00+00:00</td>\n",
       "      <td>safest_players_to_draft_favorite_lateround_tar...</td>\n",
       "      <td>G:\\My Drive\\Columbia\\Practical Deep Learning\\F...</td>\n",
       "      <td>welcome into a Wednesday edition of the RotoWi...</td>\n",
       "      <td>3911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-19 17:02:00+00:00</td>\n",
       "      <td>players_to_avoid_who_to_take_after_jokic_favor...</td>\n",
       "      <td>G:\\My Drive\\Columbia\\Practical Deep Learning\\F...</td>\n",
       "      <td>hey everybody I'm doc he's Rick where the fant...</td>\n",
       "      <td>3203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-20 16:02:30+00:00</td>\n",
       "      <td>nba_preseason_risers_fallers_202324</td>\n",
       "      <td>G:\\My Drive\\Columbia\\Practical Deep Learning\\F...</td>\n",
       "      <td>welcome to the award-winning RotoWire fantasy ...</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-23 21:58:00+00:00</td>\n",
       "      <td>western_conference_win_total_overunder_picks_w...</td>\n",
       "      <td>G:\\My Drive\\Columbia\\Practical Deep Learning\\F...</td>\n",
       "      <td>welcome to the RotoWire fantasy basketball pod...</td>\n",
       "      <td>4359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-24 17:38:23+00:00</td>\n",
       "      <td>opening_week_preview_boom_or_bust_players_thre...</td>\n",
       "      <td>G:\\My Drive\\Columbia\\Practical Deep Learning\\F...</td>\n",
       "      <td>Hello friends and welcome to the award-winning...</td>\n",
       "      <td>2664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            publication_date                                          file_name                                          file_path                                            content  duration\n",
       "0  2023-10-18 16:25:00+00:00  safest_players_to_draft_favorite_lateround_tar...  G:\\My Drive\\Columbia\\Practical Deep Learning\\F...  welcome into a Wednesday edition of the RotoWi...      3911\n",
       "1  2023-10-19 17:02:00+00:00  players_to_avoid_who_to_take_after_jokic_favor...  G:\\My Drive\\Columbia\\Practical Deep Learning\\F...  hey everybody I'm doc he's Rick where the fant...      3203\n",
       "2  2023-10-20 16:02:30+00:00                nba_preseason_risers_fallers_202324  G:\\My Drive\\Columbia\\Practical Deep Learning\\F...  welcome to the award-winning RotoWire fantasy ...      1840\n",
       "3  2023-10-23 21:58:00+00:00  western_conference_win_total_overunder_picks_w...  G:\\My Drive\\Columbia\\Practical Deep Learning\\F...  welcome to the RotoWire fantasy basketball pod...      4359\n",
       "4  2023-10-24 17:38:23+00:00  opening_week_preview_boom_or_bust_players_thre...  G:\\My Drive\\Columbia\\Practical Deep Learning\\F...  Hello friends and welcome to the award-winning...      2664"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_utils import PodcastContainer, load_clean_scores\n",
    "from src.player_utils import PlayerUtil\n",
    "from src.utils import  get_repo_root\n",
    "\n",
    "cont = PodcastContainer()\n",
    "podcast_df = cont.get_podcast_data()\n",
    "podcast_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7cfc9878-c1ee-4cc6-9a74-0a4bd8fd7d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:44 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d0f4a28d840b798eec80679bd1af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:44 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21915df4efb04925be4dd3e45be83daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:45 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for LeBron James:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45abd8a24c5b43ffac8b9b1c9229ca17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:45 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcc9f785f4a4375b01ae3b3b6f893c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:46 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for Stephen Curry:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bd88ed6b7438fb4ba75552f980f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:46 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a756e57a9e004402a56da8d959a774d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:47 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for Anthony Davis:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34479c3f779498aba9802e192df48ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:40:47 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f00aadf7ddd456da333560c06686302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for James Harden:\n",
      "  - you know some of our kind of safety valve players that we could always fall back on year in and year out but just kind of Jump Around hit some other news items I just saw a tweet Brandon indicating that James Harden is a no-show at Sixers practice today I like I still feel like we're one well\n",
      "  - least anonymously sourced yesterday you know that the Sixers are starting to Canvas a little bit more aggressively as far as you know what a James Harden trade might look like getting some other teams beyond the Clippers involved I still think the most likely scenario given that we're six days away from opening night is that hard and opens the season with the team but I don't know if he's gonna be out there\n",
      "  - at opening like I still think he'll be a member of the Sixers but do you do you expect him to play in their first game of the season I don't so I've actually changed my tune on this the last couple of days I don't think he's gonna be on the team but by the time this the season starts I know that it's coming coming up soon we're only a few days away\n",
      "  - from that but this is a situation that as James Harden has said himself does not repairable the Sixers have a high expectations this year they can't start the season with a headache like this you're not going to get great value that ship has sailed a long time ago you just have to move off of James Harden get what you can they know that and it seems like he's willing to play hardball\n",
      "  - in terms of not suiting up he said that he just wants to play basketball but I think it was with a dot dot dot implied not for you so I don't think he's gonna be on the team he'll probably be he might be traded by the time we drop this episode it's possible it's possible you know I I think the fact that it's Daryl Morey on the other side of this and\n",
      "  - he's like the one GM who I feel like has the the best track record of not being pushed around in these scenarios I I have a hard time believing that he's just going to cave and take 60 cents on the dollar for James Harden but you know it's got to the point now where I think both sides would like this to be resolved before opening night and that pressure is only going\n",
      "  - to turn up before we get to the middle of next week in the Sixers play their first game you know is there any other team that you could see emerging other than the LA Clippers like I I don't know what team is lining up to get James Harden right now I think there are some teams that are probably monitoring this and saying okay you know if we could see him play for\n",
      "  - a couple months and it's he that he's engaged and see that he's still has it you know maybe later on depending on how our Seasons going we would consider bringing a player like that in you know you think of a team like the Heat or I don't know the Lakers who are always open to bringing in any big name player but I think those teams feel pretty good about where they're at\n",
      "  - the Sixers and you say you know let's just you know for the sake of conversation you want to trade him by opening night I think my goal would be getting a third team involved because I'm not I'm just not thrilled with what I like it off here like I'm not I'm not the biggest Terrace man guy I know they're calling him like borderline on touchable I don't I get it he's looked\n",
      "  - okay you know when he's been giving the opportunity but if that's the headliner that I'm getting back for James Harden I'm not psyched about it you know it could be someone like Norman Powell who's you know a good 15 to 18 point per game guy off the bench that's fine but you know unless you get another team involved as the Sixers I don't think you're gonna get anything close to what you\n",
      "  - I think the most likely scenarios he gets dealt for it's actually if it's the Clippers and unprotected first round pick and then if you're Philadelphia you cross your fingers and hope that the Clippers end up dealing with the injury bug and that turns into a really valuable commodity but I just don't think you're going to get a player of any substance in return at this point I know it sounds crazy to\n",
      "  - say James Harden at his best is really good player there's just there's not a big market for him the seasons about the start all these rosters are locked in and the Sixers want to get rid of him a whole lot more than these other teams seem to want to bring him in I do think the Clippers make the most sense I've thought that all long Far and Away at the top of\n",
      "  - the list there are a couple of others I think Minnesota would be interesting to add another piece to to their offense I mean if the Raptors wanted to try something different I don't see them making a move like that I know people keep throwing Miami out there I think there's a less than 0% chance he ends up with the Miami Heat Jimmy Butler has had a hard time getting along with teammates\n",
      "  - over the years because because other players he's played with aren't completely locked in and committed to basketball James Harden strikes me as the kind of guy that would drive him absolutely crazy\n",
      "  - so there's no question about that I agree with you I also like part of me just as a basketball fan just out of pure curiosity I kind of want to see him in Miami to see if that's the one organization that could get the most out of him right like for the most part they've they've avoided you know these wouldn't say troubled players like that that's not really fair to James Harden\n",
      "  - but guys like hard India they tend to avoid but I think you know some of these same criticisms that were we have about James Harden right now we could have said the same thing about Jimmy Butler when he was you know forcing his way out of Minnesota on national television and you know you go to my Emmy and you kind of get with that organization you get with that coaching staff and\n",
      "  - they have a way about you know maximizing players especially players like hardened who we've kind of questioned you know what kind of shape they're in to begin Seasons like Miami does it doesn't deal with any of that you know so I I do think that there's a case to be made that if there's one team that could kind of get James Harden where he needs to be at this point in his\n",
      "  - career it is the heat but I think Miami is content to kind of sit back play this out at least until you know midseason trade deadline and that's you know when they would hopefully you know another Super are Shakespeare and potentially make their move after striking out on Damian Lillard I do want to ask you you know if you think James Harden is being dealt in the next week or so but\n",
      "  - that deal which seems unlikely does that mean that you're willing you know to push Tyrese Maxey up you know half round or maybe even a full round absolutely I think a full round sounds like the right sort of move in terms of the way his value would increase with James Harden not on the floor we thought last year was going to be that big breakout for Tyrese Maxey he really just kind\n",
      "  - of steady and stayed the same he did not have that big breakout season we were expecting without James Harden I think that statistically you would get that he's improving or you you at least think that his stage in the game that he should be taking that next step\n"
     ]
    }
   ],
   "source": [
    "text = podcast_df.iloc[0].content[:10000]\n",
    "# text[:10000]\n",
    "\n",
    "# List of people to resolve\n",
    "people = [\"LeBron James\", \"Stephen Curry\", \"Anthony Davis\", \"James Harden\"]\n",
    "\n",
    "# Find sentences containing references to each person\n",
    "for person in people:\n",
    "    coref_sentences = find_coreferences_for_person(text, person)\n",
    "    print(f\"\\nSentences for {person}:\")\n",
    "    for sentence in coref_sentences:\n",
    "        print(f\"  - {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aea01027-99e5-43d4-b33b-9c886187c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def split_sentences(text) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Splits the text into sentences using regex for better handling of punctuation.\n",
    "#     \"\"\"\n",
    "#     chunk_size = 100\n",
    "#     words = text.split()\n",
    "#     chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "#     return chunks\n",
    "\n",
    "def split_sentences(text: str, chunk_size: int = 100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into sentences by first splitting on double spaces,\n",
    "    and only further splits chunks if they exceed the specified word limit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to split.\n",
    "    chunk_size : int\n",
    "        The maximum number of words per chunk. Defaults to 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Step 1: Split the text on double spaces\n",
    "    double_space_chunks = text.split(\"  \")\n",
    "\n",
    "    # Step 2: Process each chunk, splitting only if it exceeds the chunk size\n",
    "    result = []\n",
    "    for chunk in double_space_chunks:\n",
    "        words = chunk.split()\n",
    "        if len(words) > chunk_size:\n",
    "            # Split the chunk into smaller parts\n",
    "            result.extend([' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)])\n",
    "        else:\n",
    "            # Keep the chunk as is\n",
    "            result.append(chunk.strip())\n",
    "\n",
    "    return result\n",
    "\n",
    "    \n",
    "# split_sentences(text, chunk_size=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e811f-594d-4980-adac-82c82e879908",
   "metadata": {},
   "source": [
    "## FastCoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "023d2aef-ce34-43e8-9d29-77b7048784e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:23 - INFO - \t missing_keys: []\n",
      "12/05/2024 23:39:23 - INFO - \t unexpected_keys: []\n",
      "12/05/2024 23:39:23 - INFO - \t mismatched_keys: []\n",
      "12/05/2024 23:39:23 - INFO - \t error_msgs: []\n",
      "12/05/2024 23:39:23 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "12/05/2024 23:39:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e2bce809784470be1b2edd19eeca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:23 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308eea2d18b043a4b5f6a51d4acb1330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for LeBron James:\n",
      "  - LeBron James played an incredible game last night, scoring 40 points\n",
      "  - He showed great leadership on the court\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd9527d67a447b49eb9d2ffe8b55d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:23 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d24b6c07c1049d59a4d09cf53a85310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:23 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for Stephen Curry:\n",
      "  - Meanwhile, Stephen Curry impressed everyone with his three-point shooting, hitting 7 threes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24a07e0abcb46e682c0ac691a614d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/05/2024 23:39:24 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00fd70f08934a439fbe1f0915a34f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences for Anthony Davis:\n",
      "  - Anthony Davis was solid on defense, but his offensive output was limited\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import FCoref\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the FastCoref model\n",
    "coref_model = FCoref(device='cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def resolve_coreferences(text):\n",
    "    \"\"\"\n",
    "    Resolves coreferences in the text using FastCoref.\n",
    "    \"\"\"\n",
    "    doc = coref_model.predict(texts=[text])\n",
    "    return doc[0]\n",
    "\n",
    "def find_coreferences_for_person(text, person_name):\n",
    "    \"\"\"\n",
    "    Find all sentences containing coreferences to the specified person.\n",
    "    \"\"\"\n",
    "    # Resolve coreferences in the text\n",
    "    resolved_doc = resolve_coreferences(text)\n",
    "\n",
    "    # Extract the clusters\n",
    "    clusters = resolved_doc.get_clusters(as_strings=False)\n",
    "    cluster_texts = resolved_doc.get_clusters()\n",
    "\n",
    "    # Find the cluster associated with the person\n",
    "    target_cluster = None\n",
    "    for cluster_text, cluster_spans in zip(cluster_texts, clusters):\n",
    "        if any(person_name.lower() in mention.lower() for mention in cluster_text):\n",
    "            target_cluster = cluster_spans\n",
    "            break\n",
    "\n",
    "    if not target_cluster:\n",
    "        return []  # No coreferences for the person found\n",
    "\n",
    "    # Split the text into sentences    \n",
    "    sentences = split_sentences(text, 75)\n",
    "    sentence_lengths = [len(s) for s in sentences]\n",
    "    sentence_endings = np.cumsum(sentence_lengths)\n",
    "\n",
    "    sentence_boundaries = [(0, sentence_endings[0])]\n",
    "    for i in range(1, len(sentence_endings)):\n",
    "        sentence_boundaries.append((sentence_endings[i-1]+1, sentence_endings[i]))\n",
    "\n",
    "    # Find sentences containing the coreference spans\n",
    "    sentences_with_coref = set()\n",
    "    for span in target_cluster:\n",
    "        span_start, span_end = span\n",
    "        for idx, (start, end) in enumerate(sentence_boundaries):\n",
    "            if span_start >= start and span_end <= end:\n",
    "                sentences_with_coref.add(idx)\n",
    "\n",
    "    return [sentences[i] for i in sorted(sentences_with_coref)]\n",
    "\n",
    "# Example usage\n",
    "podcast_text = \"\"\"\n",
    "LeBron James played an incredible game last night, scoring 40 points  He showed great leadership on the court  Meanwhile, Stephen Curry impressed everyone with his three-point shooting, hitting 7 threes  Anthony Davis was solid on defense, but his offensive output was limited\n",
    "\"\"\"\n",
    "\n",
    "# List of people to resolve\n",
    "people = [\"LeBron James\", \"Stephen Curry\", \"Anthony Davis\"]\n",
    "\n",
    "# Find sentences containing references to each person\n",
    "for person in people:\n",
    "    coref_sentences = find_coreferences_for_person(podcast_text, person)\n",
    "    print(f\"\\nSentences for {person}:\")\n",
    "    for sentence in coref_sentences:\n",
    "        print(f\"  - {sentence}\")\n",
    "\n",
    "\n",
    "# coref_sentences = find_coreferences_for_person(podcast_text, \"LeBron James\")\n",
    "# coref_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38b3f3e-3a20-4939-b762-bce98990fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data (for sentence tokenization)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Load SpaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def segment_by_semantics(text, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Segments text into semantically meaningful chunks based on SpaCy's parsing.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    segments = []\n",
    "    chunk = []\n",
    "    token_count = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        token_count += len(sent)\n",
    "        chunk.append(sent.text)\n",
    "        if token_count >= max_tokens:\n",
    "            segments.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "            token_count = 0\n",
    "\n",
    "    # Add any remaining sentences\n",
    "    if chunk:\n",
    "        segments.append(\" \".join(chunk))\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes irrelevant sections and prepares text for further processing.\n",
    "    \"\"\"\n",
    "    # Remove advertisements or specific unwanted patterns\n",
    "    ad_patterns = [\n",
    "        r\"(?i)sponsored by .*\",  # Example: \"Sponsored by XYZ\"\n",
    "        r\"(?i)ad break.*\",  # Example: \"Ad break starts here\"\n",
    "        r\"http\\S+\",  # URLs\n",
    "        r\"\\[.*?\\]\",  # Content in brackets (e.g., [Music])\n",
    "    ]\n",
    "    for pattern in ad_patterns:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Define a function to annotate entities\n",
    "def annotate_entities(sentences):\n",
    "    \"\"\"\n",
    "    Uses SpaCy's NER to detect and annotate entities.\n",
    "    Returns a list of sentences with annotated entities.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        annotations.append({\"sentence\": sentence, \"entities\": entities})\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Process all text files in a directory\n",
    "def process_text_files(directory):\n",
    "    \"\"\"\n",
    "    Processes all .txt files in the specified directory.\n",
    "    Cleans text, segments it into sentences, and annotates entities.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # Clean text\n",
    "            # cleaned_text = clean_text(text)\n",
    "\n",
    "            # Segment text\n",
    "            sentences = segment_by_semantics(text)\n",
    "\n",
    "            # Annotate entities\n",
    "            annotated_sentences = annotate_entities(sentences)\n",
    "\n",
    "            # Save results\n",
    "            results.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"file_name\": file_name,\n",
    "                        \"sentence\": ann[\"sentence\"],\n",
    "                        \"word_count\": len(ann[\"sentence\"].split()),\n",
    "                        \"entities\": ann[\"entities\"],\n",
    "                    }\n",
    "                    for ann in annotated_sentences\n",
    "                ]\n",
    "            )\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_text_file(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Segment text\n",
    "    sentences = segment_by_semantics(text)\n",
    "\n",
    "    # Annotate entities\n",
    "    annotated_sentences = annotate_entities(sentences)\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Save results\n",
    "    results = [\n",
    "        {\n",
    "            \"file_name\": file_name,\n",
    "            \"sentence\": ann[\"sentence\"],\n",
    "            \"word_count\": len(ann[\"sentence\"].split()),\n",
    "            \"entities\": ann[\"entities\"],\n",
    "        }\n",
    "        for ann in annotated_sentences\n",
    "    ]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Directory containing text files\n",
    "directory_path = \"../data/raw/rotowire_2023_2024\"\n",
    "\n",
    "date = \"2023-12-08\"\n",
    "file_path = (\n",
    "    directory_path\n",
    "    + \"/Fantasy Basketball Waiver Wire - Adds for Week 8 2023-24_transcript_6b67bc46-0000-2bfc-bcc0-2405887bfb7c.txt\"\n",
    ")\n",
    "\n",
    "results = process_text_file(file_path)\n",
    "\n",
    "# Convert to a DataFrame for easy analysis\n",
    "segment_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "current_season = get_nba_season(date)\n",
    "\n",
    "minutes = player_util.player_minute_stats(current_season)\n",
    "players = player_util.players_for_date(date)\n",
    "players = players.merge(minutes, on=[\"personId\", \"personName\"], how=\"left\")\n",
    "\n",
    "# players.sort_values(['games_over_5_minutes', 'avg_minutes_per_game'], ascending=False)\n",
    "filtered_players = players[\n",
    "    (players[\"games_over_5_minutes\"] >= 10) & (players[\"avg_minutes_per_game\"] >= 10)\n",
    "]\n",
    "\n",
    "print(len(players))\n",
    "print(len(filtered_players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb83b3c3-20af-427e-a2c9-47dffc2db6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah Shannon we saw like I mentioned Trey Murphy was a team's highest score for the Pelicans he still coming off the bench though  any concerns about that or do you think like either still a good chance he ends up playing 30 minutes a game off the bench maybe the Pokemon starting lineup at some point you know her Jones is still in there right now  yeah I mean that that 43 to 17 third quarter by the Lakers basically makes last night's game you know absolutely and there's no reason to really there's no big takeaways from last night's game is Susan turned into a blowout you know nothing really matters what happened with the Pelicans from there so the 22 minutes that Murphy saw last night I'm not worried about like you said you still the leading scorer for 14 points in those 22 minutesand anytime you was on the floor he was very aggressive on the offensive end you know he's played 2230 and 22 minutes in the three games he's returned from injury having some rest kind of being East back into the rotation champ be a big surprise but the fact that he already hit 30 minutes basically alleviates any concerns I would have will he be in the starting lineup I think that's a bigger question mark I don't Herbert Herbert Jones been playing so well for the Pelicans that I think they're gonna stick with them for at least a little while I mean they're 12 and 11 they had some assess the in-season tournament\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\".join([segment_df.iloc[i].sentence for i in [5, 6, 7]])\n",
    "sentence\n",
    "\n",
    "# TODO how can topic models be used?\n",
    "# Hierarchical Dirichlet Process (HDP)\n",
    "# Latent semantic Indexing (LSI or LDI)\n",
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfbc852b-f5b6-481e-845b-ee4c7599807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: (0, '0.033*inseason + 0.029*reason + 0.028*third + 0.027*teams + 0.025*injury + 0.024*right + 0.024*still + 0.022*herbert + 0.021*basically + 0.020*know')\n",
      "Topic 2: (1, '0.046*said + 0.036*hit + 0.036*blowout + 0.029*herbert + 0.029*mentioned + 0.027*thats + 0.026*stick + 0.025*end + 0.023*theyre + 0.022*would')\n",
      "Topic 3: (2, '0.048*champ + 0.040*rest + 0.036*yeah + 0.033*points + 0.027*hes + 0.026*blowout + 0.025*makes + 0.024*quarter + 0.021*ends + 0.020*mentioned')\n",
      "Topic 4: (3, '0.063*blowout + 0.040*know + 0.035*absolutely + 0.034*well + 0.033*injury + 0.031*floor + 0.031*turned + 0.029*teams + 0.026*big + 0.024*reason')\n",
      "Topic 5: (4, '0.064*said + 0.048*rotation + 0.043*stick + 0.036*night + 0.034*lakers + 0.030*quarter + 0.028*thats + 0.026*would + 0.025*three + 0.025*happened')\n",
      "Topic 6: (5, '0.055*injury + 0.032*games + 0.031*leading + 0.027*stick + 0.026*happened + 0.026*alleviates + 0.022*yeah + 0.021*points + 0.021*pelicans + 0.021*minutes')\n",
      "Topic 7: (6, '0.058*maybe + 0.044*absolutely + 0.041*returned + 0.030*last + 0.029*hit + 0.028*injury + 0.027*though + 0.027*would + 0.027*mark + 0.024*bigger')\n",
      "Topic 8: (7, '0.060*concerns + 0.038*back + 0.036*game + 0.033*stick + 0.031*either + 0.029*already + 0.025*ends + 0.023*mentioned + 0.021*end + 0.021*like')\n",
      "Topic 9: (8, '0.079*rest + 0.049*worried + 0.035*three + 0.035*assess + 0.030*end + 0.029*hit + 0.027*gon + 0.025*anytime + 0.025*happened + 0.022*little')\n",
      "Topic 10: (9, '0.044*like + 0.038*score + 0.035*games + 0.032*quarter + 0.031*surprise + 0.027*bigger + 0.026*yeah + 0.025*fact + 0.024*im + 0.023*east')\n",
      "Topic 11: (10, '0.056*games + 0.047*hit + 0.046*basically + 0.041*played + 0.040*murphy + 0.030*score + 0.030*rest + 0.026*trey + 0.025*little + 0.020*mark')\n",
      "Topic 12: (11, '0.065*rest + 0.039*reason + 0.031*well + 0.030*dont + 0.028*assess + 0.027*back + 0.026*big + 0.024*happened + 0.022*na + 0.022*tournament')\n",
      "Topic 13: (12, '0.039*thats + 0.037*though + 0.035*theres + 0.032*tournament + 0.030*ends + 0.030*champ + 0.030*nights + 0.029*jones + 0.029*well + 0.026*coming')\n",
      "Topic 14: (13, '0.049*pokemon + 0.042*nights + 0.039*absolutely + 0.037*takeaways + 0.035*hes + 0.033*basically + 0.032*leading + 0.030*like + 0.025*lakers + 0.022*stick')\n",
      "Topic 15: (14, '0.051*points + 0.032*mean + 0.031*lakers + 0.030*absolutely + 0.028*makes + 0.028*highest + 0.027*dont + 0.025*nothing + 0.024*returned + 0.024*shannon')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import HdpModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing special characters,\n",
    "    tokenizing, and removing stopwords.\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# File path to the podcast transcript\n",
    "directory_path = \"../data/raw/rotowire_2023_2024\"\n",
    "file_path = (\n",
    "    directory_path\n",
    "    + \"/Fantasy Basketball Waiver Wire - Adds for Week 8 2023-24_transcript_6b67bc46-0000-2bfc-bcc0-2405887bfb7c.txt\"\n",
    ")\n",
    "\n",
    "# Read and preprocess the text\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     text = f.read()\n",
    "\n",
    "text = sentence\n",
    "# Preprocess the text into tokenized segments\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Segment the text into smaller chunks (e.g., paragraphs or 50-token chunks)\n",
    "chunk_size = 50\n",
    "chunks = [\n",
    "    processed_text[i : i + chunk_size]\n",
    "    for i in range(0, len(processed_text), chunk_size)\n",
    "]\n",
    "\n",
    "# Prepare the data for HDP\n",
    "dictionary = Dictionary(chunks)\n",
    "corpus = [dictionary.doc2bow(chunk) for chunk in chunks]\n",
    "\n",
    "# Train the HDP model\n",
    "hdp_model = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "\n",
    "# Print the discovered topics\n",
    "topics = hdp_model.print_topics(num_topics=15, num_words=10)\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic {i + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "664857c4-9ea5-4c26-b497-cc7c8d802de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.2795729567203427\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute the coherence score\n",
    "coherence_model = CoherenceModel(\n",
    "    model=hdp_model, texts=chunks, dictionary=dictionary, coherence=\"c_v\"\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065d47d-0bdc-4b6a-a94a-b8c02ecff24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO better segmentation for better topic modeling?\n",
    "# TODO look at topic distributions across documents\n",
    "# TODO topic visualization in 2D space\n",
    "\n",
    "# import pyLDAvis.gensim_models as gensimvis\n",
    "# import pyLDAvis\n",
    "\n",
    "# # Prepare the visualization\n",
    "# vis_data = gensimvis.prepare(hdp_model, corpus, dictionary)\n",
    "# pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e61932-3ca3-4977-ba68-36af92da0e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c218202-008e-470c-90e2-40f2c2fc0d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a028543-4aad-4a65-8eaf-eacbac748f27",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1723e8dc-f22c-48f9-b6b2-544b5122f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of numpy.core.multiarray failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 1, in <module>\n",
      "    from numpy._core import multiarray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core.fromnumeric failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\umath.py\", line 2, in __getattr__\n",
      "    from numpy._core import umath\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\umath.py\", line 15, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_get_extobj_dict' from 'numpy.core._multiarray_umath' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_multiarray_umath.cp310-win_amd64.pyd)\n",
      "]\n",
      "[autoreload of numpy.core.arrayprint failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\numerictypes.py\", line 2, in __getattr__\n",
      "    from numpy._core import numerictypes\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numerictypes.py\", line 82, in <module>\n",
      "    from . import multiarray as ma\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core.numeric failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\umath.py\", line 2, in __getattr__\n",
      "    from numpy._core import umath\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\umath.py\", line 15, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_get_extobj_dict' from 'numpy.core._multiarray_umath' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_multiarray_umath.cp310-win_amd64.pyd)\n",
      "]\n",
      "[autoreload of numpy.core.records failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\numerictypes.py\", line 2, in __getattr__\n",
      "    from numpy._core import numerictypes\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numerictypes.py\", line 82, in <module>\n",
      "    from . import multiarray as ma\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core.getlimits failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\numerictypes.py\", line 2, in __getattr__\n",
      "    from numpy._core import numerictypes\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numerictypes.py\", line 82, in <module>\n",
      "    from . import multiarray as ma\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core._internal failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_internal.py\", line 1, in <module>\n",
      "    from numpy._core import _internal\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\_internal.py\", line 13, in <module>\n",
      "    from .multiarray import dtype, array, ndarray, promote_types, StringDType\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\umath.py\", line 2, in __getattr__\n",
      "    from numpy._core import umath\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\umath.py\", line 15, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_get_extobj_dict' from 'numpy.core._multiarray_umath' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_multiarray_umath.cp310-win_amd64.pyd)\n",
      "]\n",
      "[autoreload of numpy.lib.mixins failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\umath.py\", line 2, in __getattr__\n",
      "    from numpy._core import umath\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\umath.py\", line 15, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_get_extobj_dict' from 'numpy.core._multiarray_umath' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_multiarray_umath.cp310-win_amd64.pyd)\n",
      "]\n",
      "[autoreload of numpy.lib.scimath failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\numerictypes.py\", line 2, in __getattr__\n",
      "    from numpy._core import numerictypes\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numerictypes.py\", line 82, in <module>\n",
      "    from . import multiarray as ma\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.lib.stride_tricks failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\", line 1, in <module>\n",
      "    from ._stride_tricks_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_stride_tricks_impl.py\", line 14, in <module>\n",
      "    from numpy._core.numeric import normalize_axis_tuple\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numeric.py\", line 11, in <module>\n",
      "    from . import multiarray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_scalars.py:12: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  _BoolLike_co = Union[bool, np.bool]\n",
      "[autoreload of numpy._typing._scalars failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_scalars.py\", line 12, in <module>\n",
      "    _BoolLike_co = Union[bool, np.bool]\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    set(lib._utils_impl.__all__) |\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_dtype_like.py:142: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  type[np.bool],\n",
      "[autoreload of numpy._typing._dtype_like failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_dtype_like.py\", line 142, in <module>\n",
      "    type[np.bool],\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    set(lib._utils_impl.__all__) |\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_array_like.py:97: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  dtype[np.bool],\n",
      "[autoreload of numpy._typing._array_like failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_typing\\_array_like.py\", line 97, in <module>\n",
      "    dtype[np.bool],\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\__init__.py\", line 324, in __getattr__\n",
      "    set(lib._utils_impl.__all__) |\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "]\n",
      "[autoreload of numpy.linalg failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\linalg\\linalg.py\", line 3, in __getattr__\n",
      "    from numpy.linalg import _linalg\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\linalg\\_linalg.py\", line 25, in <module>\n",
      "    from numpy._core import (\n",
      "ImportError: cannot import name 'array' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.matrixlib.defmatrix failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\", line 8, in <module>\n",
      "    import numpy._core.numeric as N\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numeric.py\", line 11, in <module>\n",
      "    from . import multiarray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.lib.format failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\format.py\", line 170, in <module>\n",
      "    from numpy.lib._utils_impl import drop_metadata\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_utils_impl.py\", line 10, in <module>\n",
      "    from numpy._core import ndarray\n",
      "ImportError: cannot import name 'ndarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.lib._iotools failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_iotools.py\", line 7, in <module>\n",
      "    import numpy._core.numeric as nx\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numeric.py\", line 11, in <module>\n",
      "    from . import multiarray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.lib.npyio failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 1, in <module>\n",
      "    from ._npyio_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py\", line 20, in <module>\n",
      "    from numpy._core.multiarray import packbits, unpackbits\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.lib failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\__init__.py\", line 13, in <module>\n",
      "    from . import array_utils\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.fft._pocketfft failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\fft\\_pocketfft.py\", line 36, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.fft failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\fft\\helper.py\", line 3, in __getattr__\n",
      "    from numpy.fft import _helper\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\fft\\_helper.py\", line 5, in <module>\n",
      "    from numpy._core import integer, empty, arange, asarray, roll\n",
      "ImportError: cannot import name 'integer' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.polyutils failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\polyutils.py\", line 27, in <module>\n",
      "    from numpy._core.multiarray import dragon4_positional, dragon4_scientific\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.polynomial.polynomial failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\polynomial.py\", line 85, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.chebyshev failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\chebyshev.py\", line 112, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.legendre failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\legendre.py\", line 84, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.hermite failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\hermite.py\", line 80, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.hermite_e failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\hermite_e.py\", line 80, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.polynomial.laguerre failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\polynomial\\laguerre.py\", line 80, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy.ctypeslib failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\ctypeslib.py\", line 59, in <module>\n",
      "    from numpy._core.multiarray import _flagdict, flagsobj\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\umath.py\", line 2, in __getattr__\n",
      "    from numpy._core import umath\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\umath.py\", line 15, in <module>\n",
      "    from ._multiarray_umath import (\n",
      "ImportError: cannot import name '_get_extobj_dict' from 'numpy.core._multiarray_umath' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\_multiarray_umath.cp310-win_amd64.pyd)\n",
      "]\n",
      "[autoreload of numpy.ma.extras failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\ma\\extras.py\", line 35, in <module>\n",
      "    from numpy.lib.array_utils import normalize_axis_index, normalize_axis_tuple\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\array_utils.py\", line 1, in <module>\n",
      "    from ._array_utils_impl import (\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\lib\\_array_utils_impl.py\", line 4, in <module>\n",
      "    from numpy._core import asarray\n",
      "ImportError: cannot import name 'asarray' from 'numpy._core' (C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py)\n",
      "]\n",
      "[autoreload of numpy failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    if not append_obj(module, old_objects, name, obj):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 423, in append_obj\n",
      "    in_module = hasattr(obj, \"__module__\") and obj.__module__ == module.__name__\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\core\\defchararray.py\", line 2, in __getattr__\n",
      "    from numpy._core import defchararray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\defchararray.py\", line 21, in <module>\n",
      "    from .numerictypes import bytes_, str_, character\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\numerictypes.py\", line 82, in <module>\n",
      "    from . import multiarray as ma\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n",
      "[autoreload of numpy._core failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\__init__.py\", line 23, in <module>\n",
      "    from . import multiarray\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\multiarray.py\", line 84, in <module>\n",
      "    def empty_like(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 178, in decorator\n",
      "    return array_function_dispatch(\n",
      "  File \"C:\\Users\\Patrick\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\numpy\\_core\\overrides.py\", line 158, in decorator\n",
      "    add_docstring(implementation, dispatcher.__doc__)\n",
      "RuntimeError: empty_like method already has a different docstring\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FCoref.__init__() got an unexpected keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the FastCoref model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m coref_model \u001b[38;5;241m=\u001b[39m \u001b[43mFCoref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbiu-nlp/f-coref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load a BERT model and tokenizer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: FCoref.__init__() got an unexpected keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from fastcoref import FCoref\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the FastCoref model\n",
    "coref_model = FCoref(model_name=\"biu-nlp/f-coref\")\n",
    "\n",
    "# Load a BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def get_players():\n",
    "    # Replace with your logic to retrieve player names\n",
    "    return {\"LeBron James\", \"Stephen Curry\", \"Anthony Davis\"}\n",
    "\n",
    "\n",
    "def resolve_coreferences(text):\n",
    "    \"\"\"\n",
    "    Resolves coreferences in the text using FastCoref.\n",
    "    \"\"\"\n",
    "    doc = coref_model.predict(text, aggregation_strategy=\"average\")\n",
    "    resolved_text = doc.resolved_text\n",
    "    return resolved_text\n",
    "\n",
    "\n",
    "def extract_player_sentences(podcast_text, players):\n",
    "    \"\"\"\n",
    "    Extracts sentences mentioning specific players using coreference resolution.\n",
    "    \"\"\"\n",
    "    resolved_text = resolve_coreferences(podcast_text)\n",
    "    sentences = resolved_text.split(\". \")  # Simple sentence splitting\n",
    "\n",
    "    player_sentences = {player: [] for player in players}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if a player is mentioned directly in the sentence\n",
    "        for player in players:\n",
    "            if player in sentence:\n",
    "                player_sentences[player].append(sentence.strip())\n",
    "\n",
    "    return player_sentences\n",
    "\n",
    "\n",
    "def generate_player_vectors(player_sentences):\n",
    "    \"\"\"\n",
    "    Generate vector representations for each player using BERT.\n",
    "    \"\"\"\n",
    "    player_vectors = {}\n",
    "\n",
    "    for player, sentences in player_sentences.items():\n",
    "        # Tokenize and process all sentences related to the player\n",
    "        sentence_embeddings = []\n",
    "        for sentence in sentences:\n",
    "            inputs = tokenizer(\n",
    "                sentence, return_tensors=\"pt\", truncation=True, padding=True\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(**inputs)\n",
    "                # Use [CLS] token embedding as the representation\n",
    "                sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "                sentence_embeddings.append(sentence_embedding.numpy())\n",
    "\n",
    "        # Aggregate sentence embeddings for the player (mean pooling)\n",
    "        if sentence_embeddings:\n",
    "            player_vectors[player] = np.mean(sentence_embeddings, axis=0)\n",
    "        else:\n",
    "            player_vectors[player] = None  # No context found for the player\n",
    "\n",
    "    return player_vectors\n",
    "\n",
    "\n",
    "podcast_text = \"\"\"\n",
    "LeBron James played an incredible game last night, scoring 40 points. He showed great leadership on the court.\n",
    "Meanwhile, Stephen Curry impressed everyone with his three-point shooting, hitting 7 threes.\n",
    "Anthony Davis was solid on defense, but his offensive output was limited.\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve players of interest\n",
    "players = get_players()\n",
    "\n",
    "# Extract sentences mentioning each player\n",
    "player_sentences = extract_player_sentences(podcast_text, players)\n",
    "print(\"Player Sentences:\")\n",
    "for player, sentences in player_sentences.items():\n",
    "    print(f\"{player}: {sentences}\")\n",
    "\n",
    "# Generate vector representations\n",
    "player_vectors = generate_player_vectors(player_sentences)\n",
    "print(\"\\nPlayer Vectors:\")\n",
    "for player, vector in player_vectors.items():\n",
    "    print(f\"{player}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21134433-93bc-46ac-bb30-f3767489e193",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mextract_player_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_players\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 36\u001b[0m, in \u001b[0;36mextract_player_sentences\u001b[1;34m(podcast_text, players)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Avoid duplicate additions\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Check for coreferential mentions\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoref_clusters\u001b[49m:  \u001b[38;5;66;03m# Coreference clusters\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(player \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(mention) \u001b[38;5;28;01mfor\u001b[39;00m player \u001b[38;5;129;01min\u001b[39;00m players \u001b[38;5;28;01mfor\u001b[39;00m mention \u001b[38;5;129;01min\u001b[39;00m cluster\u001b[38;5;241m.\u001b[39mmentions):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# If player is coreferentially linked, add the sentence\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         player_sentences[player]\u001b[38;5;241m.\u001b[39mappend(sent\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310-ml-gpu\\lib\\site-packages\\spacy\\tokens\\underscore.py:48\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     49\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'coref_clusters'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "extract_player_sentences(text, get_players())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262ddbd-debc-45ad-b1eb-1d48aa3a8049",
   "metadata": {},
   "source": [
    "## Bert Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b28f5bd6-6557-4188-aada-fac4d84ebea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the podcast transcript\n",
    "directory_path = \"../data/raw/rotowire_2023_2024\"\n",
    "file_path = (\n",
    "    directory_path\n",
    "    + \"/Fantasy Basketball Waiver Wire - Adds for Week 8 2023-24_transcript_6b67bc46-0000-2bfc-bcc0-2405887bfb7c.txt\"\n",
    ")\n",
    "\n",
    "# Read and preprocess the text\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b0107-eac5-4d94-9443-218725124201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5d7cb2e-8c7f-4435-9ccb-f111cc7e603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was said about Herbert Jones performance?\n",
      "A: the thing that disturbs me is he's not a young kid\n",
      "\n",
      "Q: What was the sentiment around Herbert Jones?\n",
      "A: he's not a young kid\n",
      "\n",
      "Q: Were there any criticisms of Herbert Jones?\n",
      "A: he's got two guys on the team that are better players than him\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a QA pipeline\n",
    "# qa_pipeline = pipeline(\"question-answering\", model=\"deberta-v3-large\")\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    ")\n",
    "\n",
    "# Input context (podcast transcript)\n",
    "# context = \"LeBron James played exceptionally well last night, scoring 40 points. However, his defense was questionable...\"\n",
    "context = text\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"What was said about Herbert Jones performance?\",\n",
    "    \"What was the sentiment around Herbert Jones?\",\n",
    "    \"Were there any criticisms of Herbert Jones?\",\n",
    "]\n",
    "\n",
    "# Get answers for each question\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Q: {question}\\nA: {result['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459251ed-8565-4b02-b2c3-5516501c85db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff78731-5f76-4f5f-925a-1a75db8c5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Approaches\n",
    "# If the above adjustments still seem complex, consider alternative modeling approaches:\n",
    "\n",
    "# 1. Use a Transformer-Based Model for End-to-End Analysis\n",
    "# Models like GPT or BERT-based architectures can handle NER, context understanding, and sentiment analysis jointly.\n",
    "# Fine-tune these models on annotated podcast text for:\n",
    "# Identifying players\n",
    "# Extracting context\n",
    "# Classifying sentiment directly\n",
    "# 2. Treat it as a Question-Answering Task\n",
    "# Formulate the task as:\n",
    "# \"What is the sentiment around Player X in the podcast?\"\n",
    "# Use QA models to identify the most relevant segment and its sentiment.\n",
    "# 3. Use Context-Aware Embedding Models\n",
    "# Embed the entire podcast using a context-aware model like Sentence-BERT.\n",
    "# Use similarity-based search to locate the most relevant context around a player mention and then analyze sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861558a3-50ba-407c-bca9-4a7115e28501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving Your Plan\n",
    "# Instead of completely changing your plan, you can make it \"smarter\" and more manageable by addressing the key challenges strategically:\n",
    "\n",
    "# 1. Named Entity Recognition (NER)\n",
    "# Fine-tune or use a domain-specific model:\n",
    "\n",
    "# Use pre-trained models like SpaCy or Hugging Face Transformers.\n",
    "# Fine-tune these models on basketball-related text datasets (e.g., annotated transcripts, sports news, or player rosters).\n",
    "# Incorporate a list of basketball player names:\n",
    "\n",
    "# Use external data sources like NBA rosters or fantasy basketball datasets to build a lookup table for player names.\n",
    "# Combine rule-based methods with NER:\n",
    "# Check for matches between recognized entities and your roster list.\n",
    "# Validate contextually by checking co-occurring terms like \"team,\" \"game,\" or \"points.\"\n",
    "# 2. Context Extraction\n",
    "# Dynamic Window-Based Context:\n",
    "\n",
    "# Extract a fixed-size window of text (e.g., N sentences or tokens) before and after the player mention.\n",
    "# Experiment with window sizes to balance granularity and relevance.\n",
    "# Semantic Segmentation:\n",
    "\n",
    "# Use semantic chunking techniques (like segment_by_semantics) to split the text into coherent units.\n",
    "# Identify the chunk containing the player's mention and analyze it.\n",
    "# Dependency Parsing:\n",
    "\n",
    "# Use syntactic dependency parsing to extract clauses or phrases related to the player’s name (e.g., actions or descriptors linked to the player).\n",
    "# 3. Sentiment Analysis\n",
    "# Fine-tune a Sentiment Model for Sports:\n",
    "\n",
    "# Fine-tune a sentiment analysis model on basketball-specific text to handle domain-specific nuances like sarcasm or conditional praise.\n",
    "# Example: Use Hugging Face models with labeled sentiment data from sports articles or social media.\n",
    "# Aspect-Based Sentiment Analysis (ABSA):\n",
    "\n",
    "# Train or use pre-built ABSA models to detect sentiment specifically related to the player.\n",
    "# ABSA focuses on entities (e.g., players) and their associated sentiment directly.\n",
    "# 4. Automate and Iterate\n",
    "# Use a pipeline approach to combine NER, context extraction, and sentiment analysis:\n",
    "# Perform NER to detect player mentions.\n",
    "# Extract context dynamically using a combination of window-based and semantic chunking methods.\n",
    "# Analyze sentiment on extracted context.\n",
    "# Iteratively evaluate and refine the pipeline using test cases and manually annotated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c468c44-7068-413d-8bfe-1923123a9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah Shannon we saw like I mentioned Trey Murphy was a team's highest score for the Pelicans he still coming off the bench though  any concerns about that or do you think like either still a good chance he ends up playing 30 minutes a game off the bench maybe the Pokemon starting lineup at some point you know her Jones is still in there right now  yeah I mean that that 43 to 17 third quarter by the Lakers basically makes last night's game you know absolutely and there's no reason to really there's no big takeaways from last night's game is Susan turned into a blowout you know nothing really matters what happened with the Pelicans from there so the 22 minutes that Murphy saw last night I'm not worried about like you said you still the leading scorer for 14 points in those 22 minutesand anytime you was on the floor he was very aggressive on the offensive end you know he's played 2230 and 22 minutes in the three games he's returned from injury having some rest kind of being East back into the rotation champ be a big surprise but the fact that he already hit 30 minutes basically alleviates any concerns I would have will he be in the starting lineup I think that's a bigger question mark I don't Herbert Herbert Jones been playing so well for the Pelicans that I think they're gonna stick with them for at least a little while I mean they're 12 and 11 they had some assess the in-season tournament\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\".join([segment_df.iloc[i].sentence for i in [5, 6, 7]])\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c72412b7-e1b7-4699-ad46-7ad4680beaee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneuralcoref\u001b[39;00m\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m neuralcoref\u001b[38;5;241m.\u001b[39madd_to_pipe(nlp)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import neuralcoref\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(sentence)\n",
    "doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b3793db-78d9-4590-948b-f6fa153ee748",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sentence = annotate_entities([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c949c9-bd6a-4378-9452-820fa0566946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': \"yeah Shannon we saw like I mentioned Trey Murphy was a team's highest score for the Pelicans he still coming off the bench though  any concerns about that or do you think like either still a good chance he ends up playing 30 minutes a game off the bench maybe the Pokemon starting lineup at some point you know her Jones is still in there right now  yeah I mean that that 43 to 17 third quarter by the Lakers basically makes last night's game you know absolutely and there's no reason to really there's no big takeaways from last night's game is Susan turned into a blowout you know nothing really matters what happened with the Pelicans from there so the 22 minutes that Murphy saw last night I'm not worried about like you said you still the leading scorer for 14 points in those 22 minutesand anytime you was on the floor he was very aggressive on the offensive end you know he's played 2230 and 22 minutes in the three games he's returned from injury having some rest kind of being East back into the rotation champ be a big surprise but the fact that he already hit 30 minutes basically alleviates any concerns I would have will he be in the starting lineup I think that's a bigger question mark I don't Herbert Herbert Jones been playing so well for the Pelicans that I think they're gonna stick with them for at least a little while I mean they're 12 and 11 they had some assess the in-season tournament\",\n",
       "  'entities': [('Shannon', 'FAC'),\n",
       "   ('Trey Murphy', 'PERSON'),\n",
       "   ('Pelicans', 'NORP'),\n",
       "   ('30 minutes', 'TIME'),\n",
       "   ('Pokemon', 'PERSON'),\n",
       "   ('Jones', 'PERSON'),\n",
       "   ('43 to 17 third quarter', 'DATE'),\n",
       "   (\"last night's\", 'TIME'),\n",
       "   (\"last night's\", 'TIME'),\n",
       "   ('Susan', 'PERSON'),\n",
       "   ('Pelicans', 'NORP'),\n",
       "   ('22 minutes', 'TIME'),\n",
       "   ('Murphy', 'PERSON'),\n",
       "   ('last night', 'TIME'),\n",
       "   ('14', 'CARDINAL'),\n",
       "   ('22', 'CARDINAL'),\n",
       "   ('2230 and 22 minutes', 'TIME'),\n",
       "   ('three', 'CARDINAL'),\n",
       "   ('East', 'LOC'),\n",
       "   ('30 minutes', 'TIME'),\n",
       "   ('Herbert Herbert Jones', 'PERSON'),\n",
       "   ('Pelicans', 'NORP'),\n",
       "   ('12', 'CARDINAL'),\n",
       "   ('11', 'CARDINAL')]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c45c462-7be1-4330-8ad5-4e8cebd25c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate_entities(segment_by_semantics(sentence)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404f308d-8ae1-4221-bc22-f57117f5ad77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah I mean that that 43 to 17 third quarter by the Lakers basically makes last night's game you know absolutely and there's no reason to really there's no big takeaways from last night's game is Susan turned into a blowout you know nothing really matters what happened with the Pelicans from there so the 22 minutes that Murphy saw last night I'm not worried about like you said you still the leading scorer for 14 points in those 22 minutes\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_df.iloc[6].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc3839e-33a9-41f4-84d8-07dc40ff6c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"and anytime you was on the floor he was very aggressive on the offensive end you know he's played 2230 and 22 minutes in the three games he's returned from injury having some rest kind of being East back into the rotation champ be a big surprise but the fact that he already hit 30 minutes basically alleviates any concerns I would have will he be in the starting lineup I think that's a bigger question mark I don't Herbert Herbert Jones been playing so well for the Pelicans that I think they're gonna stick with them for at least a little while I mean they're 12 and 11 they had some assess the in-season tournament\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_df.iloc[7].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae87d78-a070-4d7f-86ac-c1fc38b9bbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76b3f7-6083-418f-90e1-b18ddede37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a125558-010c-47b2-ac86-fcf8c9d0dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df.iloc[0].entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
